# 战争部分的demo设计

- 20230711
- 萨莉亚
- 20230716
- 立德-M记



## 兵种设计

三种能够互相克制的兵种：

1. 骑兵 ： 高机动性 高攻击 昂贵 克制弓箭手 被长矛兵克制
2. 长矛兵： 低机动性 低攻击 便宜 克制骑兵 被弓箭手克制
3. 弓箭手： 低机动性 高攻击 昂贵 克制长矛兵 被骑兵克制

## 环境设置

参考文明的战旗模式



不同的地形带来不同的影响，每个兵种有一些适合的地形。 ——这个还是过于复杂





### 设计阶段

我把兵种数量输入进去，然后生成出兵，就可以模拟战斗环节。

这个地方 



### 战斗机制



## AI设计

建议模式识别和强化学习两手准备。模式识别做baseline，强化学习先通过和模式识别AI对战进行学习。 

### AI父类

接受指令- 长线的，目标式的指令，而不是动作式的指令



### 模式识别AI

模式识别AI的上限更低，所以可以给更多的观测信息。 更接近一个专家模型。 

- 基础的排兵布阵： eg. 步兵在前，远程在后，骑兵放在两翼机动
- 路线规划： 内置一个A*算法 或者别的什么算法
- 战斗时机的抉择： poke , trap ， etc. 

### 可更新参数AI-Auto_regressive

#### 游戏玩法（这是0716最新kernel版）

游戏玩法：两方玩家进入游戏后分配固定个数的兵种，然后由AI控制单位进行对战。 

直观效果： 一个可以进行军队编组，军队路径规划，阵型控制，攻击目标选择的AI

输入:={己方军队坐标+相对位置，对方军队坐标+相对位置} $\times$k

输出：={决策变量：有限维的向量}

损失函数： =己方和对方实力对比+f(相对距离)+g(军队相对位置编码)+正则化项 f,g都是可微连续的启发函数

训练过程： GAN-base的架构，同时训练两个相同模型进行对抗，但是采用轮流训练的方式。

数据： 从环境中采样；记录玩家玩游戏的数据；

数据格式： 一场游戏是连续的若干帧，每次抽取若干k连续帧，**自回归预测**后m帧，然后计算损失函数。 

备用训练方法：copy参数法（把正在训练的参数直接copy给对手，moco还是哪篇对比学习论文用到了这种方法，这个对抗的主要用意在于缓慢更新）

预期效果： 能够进行一些相对合理的阵型划分，发挥**兵种配合优势**，达成多样化的战役生成。 

主要卖点： 解放玩家微操的双手，表现AI的创造力，渲染出多样化的战斗场景。 

参考游戏： 红警/实时战争模拟/文明/arena

主要架构： 对抗训练网络/时序预测模型/强化学习（待定）

#### 改进目标

通过池化层来处理变长的军队序列，然后输出一个合适的决策

输入的参数:=地形参数 军队的位置参数 建筑之间的相对参数

#### 最高目标

- 处理可变长的军队数量
- 探索模型不可观测的，但是环境存在的兵种组合效果（也就是说，AI不知道这个效果，但是能得到组合增益的反馈，然后组合增益是环境计算得到的，是作者编程实现的）
- 进行更加复杂的军队战阵的操控，比如同时处理5-10个单位
- 增加不同的地形效果，同样是游戏引擎判定，由AI学习



### 卖点

1. 让AI学习如何排兵布阵——把微操交给AI
2. 返回开放解决的战斗场面——满足视觉刺激
